{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/nirajdalavi/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nirajdalavi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nirajdalavi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "python version: 3.12.8\n",
    "beautifulsoup4==4.12.2\n",
    "bs4==0.0.2\n",
    "gensim==4.3.3\n",
    "joblib==1.4.2\n",
    "nltk==3.9.1\n",
    "numpy==1.26.4\n",
    "pandas==2.2.3\n",
    "regex==2024.11.6\n",
    "scikit-learn==1.6.1\n",
    "scipy==1.13.1\n",
    "soupsieve==2.5\n",
    "threadpoolctl==3.5.0\n",
    "torch==2.6.0\n",
    "torchaudio==2.6.0\n",
    "torchvision==0.21.0\n",
    "tqdm==4.67.1\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_frame = pd.read_csv(\"data.tsv\", sep=\"\\t\", on_bad_lines='skip', low_memory=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fc/t8b4fmzd1vn4x8ky425rp7l40000gn/T/ipykernel_25098/3038514753.py:5: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  balanced_df = pd_frame.groupby(\"star_rating\").apply(lambda x: x.sample(n=50000, random_state=42)).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "pd_frame = pd_frame[['review_body', 'star_rating']] \n",
    "pd_frame=pd_frame.dropna(subset=['star_rating','review_body'], how='any')\n",
    "pd_frame['star_rating'] = pd.to_numeric(pd_frame['star_rating'], errors='coerce')\n",
    "\n",
    "balanced_df = pd_frame.groupby(\"star_rating\").apply(lambda x: x.sample(n=50000, random_state=42)).reset_index(drop=True)\n",
    "balanced_df['sentiment'] = balanced_df['star_rating'].apply(\n",
    "    lambda x: 1 if x > 3 else (2 if x <= 2 else 3)\n",
    ")\n",
    "\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doctor - Hospital + Court = [('judge', 0.6122931838035583)]\n",
      "Similarity between 'happy' and 'satisfied': 0.6437949\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "pre_model = api.load('word2vec-google-news-300')\n",
    "result = pre_model.most_similar(positive=[\"doctor\", \"court\"], negative=[\"hospital\"], topn=1)\n",
    "print(\"Doctor - Hospital + Court =\", result)\n",
    "\n",
    "similarity = pre_model.similarity(\"happy\", \"satisfied\")\n",
    "print(\"Similarity between 'happy' and 'satisfied':\", similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doctor - Hospital + Court = [('documenting', 0.4553646147251129)]\n",
      "Similarity between 'happy' and 'satisfied' (Custom Model): 0.9003303\n"
     ]
    }
   ],
   "source": [
    "custom_mod = Word2Vec(sentences=balanced_df['review_body'].apply(word_tokenize), vector_size=300, window=11, min_count=10, workers=1, seed=43)\n",
    "\n",
    "result_custom = custom_mod.wv.most_similar(positive=[\"doctor\", \"court\"], negative=[\"hospital\"], topn=1)\n",
    "print(\"Doctor - Hospital + Court =\", result_custom)\n",
    "\n",
    "similarity_custom = custom_mod.wv.similarity(\"happy\", \"satisfied\")\n",
    "print(\"Similarity between 'happy' and 'satisfied' (Custom Model):\", similarity_custom)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing from HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "contract_dict = {\n",
    "    \"he'll\": \"he will\",\"he'll've\": \"he will have\",\"isn't\": \"is not\",\"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\"it'll\": \"it will\",\"it'll've\": \"it will have\",\"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"mightn't\": \"might not\",\"might've\": \"might have\",\n",
    "    \"must've\": \"must have\",\"mustn't\": \"must not\",\"needn't\": \"need not\",\"ain't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\"can't\": \"cannot\",\"couldn't\": \"could not\",\"could've\": \"could have\",\n",
    "    \"couldn't've\": \"could not have\",\"didn't\": \"did not\",\"doesn't\": \"does not\",\"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\"haven't\": \"have not\",\n",
    "    \"haven't've\": \"have not have\",\"he'd\": \"he would\",\"he'd've\": \"he would have\",\"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\"shalln't\": \"shall not\",\"shan't\": \"shall not\",\"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\"he's\": \"he is\",\"how'd\": \"how did\",\"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\"how's\": \"how is\",\"I'd\": \"I would\",\"I'd've\": \"I would have\",\n",
    "    \"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\"I'd\": \"I had\",\n",
    "    \"I'd've\": \"I had have\",\"I'm\": \"I am\",\"I've\": \"I have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\"so's\": \"so is\",\"that'd\": \"that would\",\"that'd've\": \"that would have\",\"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\"there'd've\": \"there would have\",\"there's\": \"there is\",\"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\"weren't\": \"were not\",\n",
    "    \"what'd\": \"what did\",\"what'd'y\": \"what do you\",\"what'll\": \"what will\",\"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\"what's\": \"what is\",\"where've\": \"where have\",\"who'd\": \"who would\",\n",
    "    \"who'd've\": \"who would have\",\"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\"why'd\": \"why did\",\"why'll\": \"why will\",\"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\"you'd\": \"you would\",\"you'd've\": \"you would have\",\"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\"you're\": \"you are\",\"you've\": \"you have\",\"what've\": \"what have\",\n",
    "    \"when'd\": \"when did\",\"when'll\": \"when will\",\"when's\": \"when is\",\"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\"where'll\": \"where will\",\"where's\": \"where is\",\n",
    "}\n",
    "\n",
    "def review_cleaning(review):\n",
    "    review = review.lower()\n",
    "    if \"<\" in review and \">\" in review: \n",
    "        review = BeautifulSoup(review, \"html.parser\").get_text()\n",
    "    review = re.sub(r'http\\S+|www\\S+', '', review)\n",
    "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in contract_dict.keys()) + r')\\b')\n",
    "    review = pattern.sub(lambda x: contract_dict[x.group()], review)\n",
    "    review = re.sub(r'[^a-zA-Z\\s]', '', review)\n",
    "    review = re.sub(r'\\s+', ' ', review).strip()\n",
    "    return review\n",
    "\n",
    "balanced_df['review_body'] = balanced_df['review_body'].astype(str)\n",
    "balanced_df['review_body'] = balanced_df['review_body'].apply(review_cleaning)\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def stopwords_removal(review):\n",
    "    words = review.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    review = ' '.join(words)\n",
    "    return review\n",
    "\n",
    "balanced_df['review_body'] = balanced_df['review_body'].apply(stopwords_removal)\n",
    "\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def review_lemmatization(review):\n",
    "    words = review.split()\n",
    "    words = [lemma.lemmatize(word) for word in words]\n",
    "    review = ' '.join(words)\n",
    "    return review\n",
    "\n",
    "\n",
    "\n",
    "balanced_df['review_body'] = balanced_df['review_body'].apply(review_lemmatization)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_df = balanced_df[balanced_df['sentiment'] != 3]\n",
    "\n",
    "X_train_bin, X_test_bin, y_train_bin, y_test_bin = train_test_split(binary_df['review_body'], binary_df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_token = [word_tokenize(text) for text in X_train_bin]\n",
    "X_test_token = [word_tokenize(text) for text in X_test_bin]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "tf_idf.fit(binary_df['review_body'])\n",
    "X_train_tfidf = tf_idf.transform(X_train_bin)\n",
    "X_test_tfidf = tf_idf.transform(X_test_bin)     \n",
    "\n",
    "def avg_vector(review, model, vector_size=300):\n",
    "    vectors = [model[word] for word in review if word in model]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(vector_size)  \n",
    "\n",
    "\n",
    "X_train_bin_avg_pre = np.array([avg_vector(review, pre_model) for review in X_train_token])\n",
    "X_test_bin_avg_pre = np.array([avg_vector(review, pre_model) for review in X_test_token])\n",
    "\n",
    "X_train_bin_avg_cus = np.array([avg_vector(review, custom_mod.wv) for review in X_train_token])\n",
    "X_test_bin_avg_cus = np.array([avg_vector(review, custom_mod.wv) for review in X_test_token])\n",
    "\n",
    "def train_eval(X_train, X_test, y_train, y_test, f_type):\n",
    "\n",
    "    perceptron = Perceptron(max_iter=1000,eta0=0.01, random_state=42)\n",
    "    perceptron.fit(X_train, y_train)\n",
    "    y_pred_perceptron = perceptron.predict(X_test)\n",
    "    perceptron_acc = accuracy_score(y_test, y_pred_perceptron)\n",
    " \n",
    "    svm = LinearSVC()\n",
    "    svm.fit(X_train, y_train)\n",
    "    y_pred_svm = svm.predict(X_test)\n",
    "    svm_acc = accuracy_score(y_test, y_pred_svm)\n",
    "\n",
    "    print(f\"Feature: {f_type}\")\n",
    "    print(f\"Perceptron Accuracy: {perceptron_acc:.4f}\")\n",
    "    print(f\"SVM Accuracy: {svm_acc:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: TF-IDF\n",
      "Perceptron Accuracy: 0.8150\n",
      "SVM Accuracy: 0.8637\n",
      "\n",
      "Feature: Word2Vec-Google-News-300\n",
      "Perceptron Accuracy: 0.7544\n",
      "SVM Accuracy: 0.8161\n",
      "\n",
      "Feature: Custom Word2Vec\n",
      "Perceptron Accuracy: 0.7825\n",
      "SVM Accuracy: 0.8435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_eval(X_train_tfidf, X_test_tfidf, y_train_bin, y_test_bin, \"TF-IDF\")\n",
    "train_eval(X_train_bin_avg_pre, X_test_bin_avg_pre, y_train_bin, y_test_bin, \"Word2Vec-Google-News-300\")\n",
    "train_eval(X_train_bin_avg_cus, X_test_bin_avg_cus, y_train_bin, y_test_bin, \"Custom Word2Vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        self.fc3 = nn.Linear(10, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return self.fc3(x)  \n",
    "\n",
    "def train_mlp(X_train, y_train, X_test, y_test, input_size, output_size, model_name):\n",
    "    mod = MLP(input_size, output_size).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(mod.parameters(), lr=0.001)\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = mod(batch_X)\n",
    "            loss = loss_fn(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_outputs = mod(X_test)\n",
    "        predictions = torch.argmax(test_outputs, dim=1)\n",
    "        accuracy = (predictions == y_test).float().mean().item()\n",
    "    \n",
    "    print(f\"{model_name}: {accuracy:.4f}\")\n",
    "\n",
    "def prep_tensors(X, y, shift_labels=True):\n",
    "    y_tensor = torch.tensor(y.values, dtype=torch.long)\n",
    "    if shift_labels:  \n",
    "        y_tensor -= 1  \n",
    "    return torch.tensor(X, dtype=torch.float32).to(device), y_tensor.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prep for ternary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ter, X_test_ter, y_train_ter, y_test_ter = train_test_split(balanced_df['review_body'], balanced_df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_ter_token = [word_tokenize(text) for text in X_train_ter]\n",
    "X_test_ter_token = [word_tokenize(text) for text in X_test_ter]\n",
    "\n",
    "X_train_ter_avg_pre = np.array([avg_vector(review, pre_model) for review in X_train_ter_token])\n",
    "X_test_ter_avg_pre = np.array([avg_vector(review, pre_model) for review in X_test_ter_token])\n",
    "\n",
    "X_train_ter_avg_cus = np.array([avg_vector(review, custom_mod.wv) for review in X_train_ter_token])\n",
    "X_test_ter_avg_cus = np.array([avg_vector(review, custom_mod.wv) for review in X_test_ter_token])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average w2v (part a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AVG custom-binary\n",
    "X_train_avg_bin_tensor, y_train_bin_tensor = prep_tensors(X_train_bin_avg_cus, y_train_bin)\n",
    "X_test_avg_bin_tensor, y_test_bin_tensor = prep_tensors(X_test_bin_avg_cus, y_test_bin)\n",
    "\n",
    "#AVG pretrained-binary\n",
    "X_train_avg_google_tensor, y_train_google_tensor = prep_tensors(X_train_bin_avg_pre, y_train_bin)\n",
    "X_test_avg_google_tensor, y_test_google_tensor = prep_tensors(X_test_bin_avg_pre, y_test_bin)\n",
    "\n",
    "#AVG custom-ternary\n",
    "X_train_avg_ter_tensor, y_train_ter_tensor = prep_tensors(X_train_ter_avg_cus, y_train_ter)\n",
    "X_test_avg_ter_tensor, y_test_ter_tensor = prep_tensors(X_test_ter_avg_cus, y_test_ter)\n",
    "\n",
    "#AVG pretrained-ternary\n",
    "X_train_avg_google_ter_tensor, y_train_google_ter_tensor = prep_tensors(X_train_ter_avg_pre, y_train_ter)\n",
    "X_test_avg_google_ter_tensor, y_test_google_ter_tensor = prep_tensors(X_test_ter_avg_pre, y_test_ter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary - Avg W2V (Google): 0.8434\n"
     ]
    }
   ],
   "source": [
    "train_mlp(X_train_avg_google_tensor, y_train_google_tensor, X_test_avg_google_tensor, y_test_google_tensor, 300, 2, \"Binary - Avg W2V (Google)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary - Avg Custom W2V: 0.8625\n"
     ]
    }
   ],
   "source": [
    "train_mlp(X_train_avg_bin_tensor, y_train_bin_tensor, X_test_avg_bin_tensor, y_test_bin_tensor, 300, 2, \"Binary - Avg Custom W2V\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ternary - Avg W2V (Google): 0.6860\n"
     ]
    }
   ],
   "source": [
    "train_mlp(X_train_avg_google_ter_tensor, y_train_google_ter_tensor, X_test_avg_google_ter_tensor, y_test_google_ter_tensor, 300, 3, \"Ternary - Avg W2V (Google)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ternary - Avg Custom W2V: 0.7031\n"
     ]
    }
   ],
   "source": [
    "train_mlp(X_train_avg_ter_tensor, y_train_ter_tensor, X_test_avg_ter_tensor, y_test_ter_tensor, 300, 3, \"Ternary - Avg Custom W2V\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenated w2v (part b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_vector(review, model, vector_size=300, max_words=10):\n",
    "    vectors = [model[word] for word in review if word in model][:max_words]  \n",
    "    while len(vectors) < max_words:  \n",
    "        vectors.append(np.zeros(vector_size))\n",
    "    return np.concatenate(vectors)\n",
    "\n",
    "X_train_bin_concat_pre = np.array([concat_vector(review, pre_model) for review in X_train_token])\n",
    "X_test_bin_concat_pre = np.array([concat_vector(review, pre_model) for review in X_test_token])\n",
    "\n",
    "X_train_bin_concat_cus = np.array([concat_vector(review, custom_mod.wv) for review in X_train_token])\n",
    "X_test_bin_concat_cus = np.array([concat_vector(review, custom_mod.wv) for review in X_test_token])\n",
    "\n",
    "X_train_ter_concat_pre = np.array([concat_vector(review, pre_model) for review in X_train_ter_token])\n",
    "X_test_ter_concat_pre = np.array([concat_vector(review, pre_model) for review in X_test_ter_token])\n",
    "\n",
    "X_train_ter_concat_cus = np.array([concat_vector(review, custom_mod.wv) for review in X_train_ter_token])\n",
    "X_test_ter_concat_cus = np.array([concat_vector(review, custom_mod.wv) for review in X_test_ter_token])\n",
    "\n",
    "#CONCAT custom-binary\n",
    "X_train_concat_bin_tensor, y_train_bin_tensor = prep_tensors(X_train_bin_concat_cus, y_train_bin)\n",
    "X_test_concat_bin_tensor, y_test_bin_tensor = prep_tensors(X_test_bin_concat_cus, y_test_bin)\n",
    "\n",
    "#CONCAT pretrained-binary\n",
    "X_train_concat_google_tensor, y_train_google_tensor = prep_tensors(X_train_bin_concat_pre, y_train_bin)\n",
    "X_test_concat_google_tensor, y_test_google_tensor = prep_tensors(X_test_bin_concat_pre, y_test_bin)\n",
    "\n",
    "#CONCAT custom-ternary\n",
    "X_train_concat_ter_tensor, y_train_ter_tensor = prep_tensors(X_train_ter_concat_cus, y_train_ter)\n",
    "X_test_concat_ter_tensor, y_test_ter_tensor = prep_tensors(X_test_ter_concat_cus, y_test_ter)\n",
    "\n",
    "#CONCAT pretrained-ternary\n",
    "X_train_concat_google_ter_tensor, y_train_google_ter_tensor = prep_tensors(X_train_ter_concat_pre, y_train_ter)\n",
    "X_test_concat_google_ter_tensor, y_test_google_ter_tensor = prep_tensors(X_test_ter_concat_pre, y_test_ter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary - concat W2V (Google): 0.7531\n"
     ]
    }
   ],
   "source": [
    "train_mlp(X_train_concat_google_tensor, y_train_google_tensor, X_test_concat_google_tensor, y_test_google_tensor, 3000, 2, \"Binary - concat W2V (Google)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary - concat Custom W2V: 0.7735\n"
     ]
    }
   ],
   "source": [
    "train_mlp(X_train_concat_bin_tensor, y_train_bin_tensor, X_test_concat_bin_tensor, y_test_bin_tensor, 3000, 2, \"Binary - concat Custom W2V\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ternary - concat W2V (Google): 0.5983\n"
     ]
    }
   ],
   "source": [
    "train_mlp(X_train_concat_google_ter_tensor, y_train_google_ter_tensor, X_test_concat_google_ter_tensor, y_test_google_ter_tensor, 3000, 3, \"Ternary - concat W2V (Google)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ternary - concat Custom W2V: 0.6183\n"
     ]
    }
   ],
   "source": [
    "train_mlp(X_train_concat_ter_tensor, y_train_ter_tensor, X_test_concat_ter_tensor, y_test_ter_tensor, 3000, 3, \"Ternary - concat Custom W2V\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Word2Vec embeddings\n",
    "def w2c_embedding(tokens, model, max_len=50, vector_size=300):\n",
    "    f_vectors = [model[word] if word in model else np.zeros(vector_size, dtype=np.float32) for word in tokens]\n",
    "    if len(f_vectors) < max_len:\n",
    "        f_vectors.extend([np.zeros(vector_size, dtype=np.float32)] * (max_len - len(f_vectors)))\n",
    "    return np.array(f_vectors[:max_len], dtype=np.float32) \n",
    "\n",
    "# PyTorch Dataset Class\n",
    "class SADataset(Dataset):\n",
    "    def __init__(self, review_tokens, labels, model):\n",
    "        self.review_tokens = review_tokens  \n",
    "        self.labels = [label - 1 for label in labels]  # zero based indexing\n",
    "        self.model = model.wv if hasattr(model, \"wv\") else model  # Word2Vec Model\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.review_tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.review_tokens[idx] \n",
    "        embedding = w2c_embedding(tokens, self.model) \n",
    "        label = self.labels[idx]  \n",
    "        return torch.tensor(embedding, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "train_bin_pre = SADataset(X_train_token, y_train_bin, pre_model)\n",
    "test_bin_pre = SADataset(X_test_token, y_test_bin, pre_model)\n",
    "train_bin_cus = SADataset(X_train_token, y_train_bin, custom_mod.wv)\n",
    "test_bin_cus = SADataset(X_test_token, y_test_bin, custom_mod.wv)\n",
    "train_ter_pre = SADataset(X_train_ter_token, y_train_ter, pre_model)\n",
    "test_ter_pre = SADataset(X_test_ter_token, y_test_ter, pre_model)\n",
    "train_ter_cus = SADataset(X_train_ter_token, y_train_ter, custom_mod.wv)\n",
    "test_ter_cus = SADataset(X_test_ter_token, y_test_ter, custom_mod.wv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_channels=300, op_size=3):  # 3 classes for ternary \n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_channels, out_channels=50, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=50, out_channels=10, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        sam_ip = torch.zeros(1, input_channels, 50)  \n",
    "        sam_op = self._get_conv_output(sam_ip)\n",
    "        self.fc1 = nn.Linear(sam_op, op_size)  \n",
    "\n",
    "    def _get_conv_output(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        return x.view(x.shape[0], -1).shape[1]  # Flatten size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  #(batch, channel, sequence)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.shape[0], -1)  # Flatten before fc1\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "#Train CNN Model with DataLoader \n",
    "def train_cnn(train_loader, test_loader, num_classes, epochs=10, lr=0.001):\n",
    "    model = CNN(op_size=num_classes).to(device)  \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)  \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)  \n",
    "            loss = loss_fn(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)  \n",
    "            outputs = model(batch_X)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"{accuracy:.4f}\")\n",
    "    return accuracy\n",
    "\n",
    "# Dataloaders for binary and ternary classification\n",
    "train_bin_cus_loader = DataLoader(train_bin_cus, batch_size=64, shuffle=True)\n",
    "test_bin_cus_loader = DataLoader(test_bin_cus, batch_size=64, shuffle=False)\n",
    "train_bin_pre_loader = DataLoader(train_bin_pre, batch_size=64, shuffle=True)\n",
    "test_bin_pre_loader = DataLoader(test_bin_pre, batch_size=64, shuffle=False)\n",
    "train_ter_cus_loader = DataLoader(train_ter_cus, batch_size=64, shuffle=True)\n",
    "test_ter_cus_loader = DataLoader(test_ter_cus, batch_size=64, shuffle=False)\n",
    "train_ter_pre_loader = DataLoader(train_ter_pre, batch_size=64, shuffle=True)\n",
    "test_ter_pre_loader = DataLoader(test_ter_pre, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training CNN for Binary Classification\n",
      "\n",
      "Accuracy(pretrained model with CNN): \n",
      "0.8610\n",
      "\n",
      "Accuracy(custom model with CNN):\n",
      "0.8642\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining CNN for Binary Classification\")\n",
    "print(\"\\nAccuracy(pretrained model with CNN): \")\n",
    "accuracy_cnn_bin_pre = train_cnn(train_bin_pre_loader, test_bin_pre_loader, num_classes=2)\n",
    "print(\"\\nAccuracy(custom model with CNN):\")\n",
    "accuracy_cnn_bin_cus = train_cnn(train_bin_cus_loader, test_bin_cus_loader, num_classes=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training CNN for Ternary Classification\n",
      "\n",
      "Accuracy(pretrained model with CNN):\n",
      "0.7005\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining CNN for Ternary Classification\")\n",
    "print(\"\\nAccuracy(pretrained model with CNN):\")\n",
    "accuracy_cnn_tern_pre = train_cnn(train_ter_pre_loader, test_ter_pre_loader, num_classes=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy(custom model with CNN):\n",
      "0.7069\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAccuracy(custom model with CNN):\")\n",
    "accuracy_cnn_tern_cus = train_cnn(train_ter_cus_loader, test_ter_cus_loader, num_classes=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
